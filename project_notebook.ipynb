{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DS501 Class Project**\n",
    "**Authors:** Daniel Alvarado Segura and Jun Lee\n",
    "\n",
    "**Description**: In this project we plan to merge two data sets which include housing selling prices for California. With this new merged data set we will be able to find a much more refined image of the house market in the area of California. \n",
    "\n",
    "**Data Sets**: We will use two open source data sets which inlcude information about the housing market of the area of California. The first set contains information of the houses per quadrant based on longitude and latitude. The second set has information of specific houses and their specific address. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Loading and Cleaning**\n",
    "**Process:** To be able to merge this two data bases we must first clean the data by dropping any rows with key information such as its address/location and house features such as number of rooms and proximity to the coast. Additionally, the second data set is split between a training set and a testing set. We will concatenate the contents of these two, to then be able to merge it with the first data set. Additionally, for the purpose of finding the information we want we will drop any rows with data that do not belong to the counties of Los Angeles, San Diego, Orange, Riverside, and San Bernardino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Framework:** Our data processing framework is composed of 4 main components, and depending on the dataset we may add others. Here is a small explanation of the main components:\n",
    "\n",
    "1. Required data definition (main funtion): The way our data processing/cleaning works is we specify which counties we want informatio from. Furthermore, in case you have a set that was divided in subsets, we can put them back together (assuming they have the same features) and specify which features to be used to compare with the other database. \n",
    "\n",
    "2. Set completion (complete_set# function): In this function we do the preliminary data cleaning necesary to execute the next 2 main components. This is tailored to our datasets, but it can easily be changed as it is it's own function.\n",
    "\n",
    "3. Find county (get_county funtion): With this function we utilize Geocoding to find the county each row of data belongs to. This allows us to filter out data from counties we don't want to analyze. In the case of the second dataset there is an additional step before this one, which is get_coordinates that finds the longitude and latitude of each address to then use this fucntion.\n",
    "\n",
    "4. County Filter (county_filter funtion): This function drops rows from counties we don't want to analyze. Said counties are decided beforehand and are provided to the function via a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_38292\\3869539251.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#Necesary modules\n",
    "import pandas as pd\n",
    "import requests\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get county from geocode\n",
    "def get_county(latitude, longitude):\n",
    "    try:\n",
    "        geocode_api = f\"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={longitude}&y={latitude}&benchmark=Public_AR_Census2020&vintage=Census2020_Census2020&layers=6&format=json\"\n",
    "        response = requests.get(geocode_api)\n",
    "        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "        geocode_api_data = response.json()\n",
    "        county_code = geocode_api_data['result']['geographies']['Census Tracts'][0]['COUNTY']\n",
    "        return county_code\n",
    "    except (requests.RequestException, KeyError, IndexError) as e:\n",
    "        print(f\"Error occurred for coordinates ({longitude}, {latitude}): {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter data from wanted counties\n",
    "def county_filter(filtered_data, county_names):\n",
    "    # Iterate over each county code in the 'county' column\n",
    "    indices_to_drop = []  # List to store indices of rows to drop\n",
    "    for index, county_code in filtered_data['county'].items():\n",
    "        # Check if the county code exists in the dictionary keys\n",
    "        if county_code in county_names:\n",
    "            # Replace the county code with the corresponding county name\n",
    "            filtered_data.at[index, 'county'] = county_names[county_code]\n",
    "        else:\n",
    "            # Add the index to the list of indices to drop\n",
    "            indices_to_drop.append(index)\n",
    "\n",
    "        # Drop rows with indices from the list and round values\n",
    "        filtered_data.drop(indices_to_drop, inplace=True)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get geocode (longitude and latitude) from address\n",
    "def get_coordinates(address, city, zip_code, state):\n",
    "    # Format the address for the API request\n",
    "    formatted_address = f\"{address.replace(' ', '+')},+{city.replace(' ', '+')},+{state}+{zip_code}\"\n",
    "    formatted_address = formatted_address.replace('#','')\n",
    "    geocoding_url = f\"https://geocoding.geo.census.gov/geocoder/locations/onelineaddress?address={formatted_address}&benchmark=2020&format=json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(geocoding_url)\n",
    "        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "        geocoding_data = response.json()\n",
    "        \n",
    "        # Extract latitude and longitude from the API response\n",
    "        if 'result' in geocoding_data and 'addressMatches' in geocoding_data['result'] and len(geocoding_data['result']['addressMatches']) > 0:\n",
    "            latitude = geocoding_data['result']['addressMatches'][0]['coordinates']['y']\n",
    "            longitude = geocoding_data['result']['addressMatches'][0]['coordinates']['x']\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred while getting geocode for address {formatted_address}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to aggregate desired columns in dataset 2\n",
    "def data_aggregation(full_data,aggregation_functions):\n",
    "    mod_full_data = full_data.groupby(['longitude', 'latitude']).agg(aggregation_functions).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading of First Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to process a dataSet\n",
    "def complete_set1(file_path):\n",
    "    coordinate_based_data = pd.read_csv(file_path)\n",
    "    cleaned_coordinate_based_data = coordinate_based_data.dropna(subset=['longitude', 'latitude'])\n",
    "    filtered_data = cleaned_coordinate_based_data[(cleaned_coordinate_based_data['longitude'] > -118.96) &\n",
    "                                                  (cleaned_coordinate_based_data['latitude'] < 35.82)]\n",
    "\n",
    "    duplicate_coords = filtered_data.groupby(['longitude', 'latitude']).size() > 1\n",
    "    for coords, is_duplicate in duplicate_coords.items():\n",
    "        if is_duplicate:\n",
    "            avg_values = filtered_data.loc[(filtered_data['longitude'] == coords[0]) &\n",
    "                                           (filtered_data['latitude'] == coords[1])].select_dtypes(include='number').mean()\n",
    "            for col in avg_values.index:\n",
    "                filtered_data.loc[(filtered_data['longitude'] == coords[0]) &\n",
    "                                  (filtered_data['latitude'] == coords[1]), col] = avg_values[col]\n",
    "    filtered_data.drop_duplicates(subset=['longitude', 'latitude'], keep='first', inplace=True)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "#File path to dataSet\n",
    "file_path = \"local_path/California_House_Info.csv\"\n",
    "\n",
    "# Define County codes wanted\n",
    "county_names = {\n",
    "    '037': 'Los Angeles',\n",
    "    '073': 'San Diego',\n",
    "    '059': 'Orange',\n",
    "    '065': 'Riverside',\n",
    "    '071': 'San Bernardino'\n",
    "}\n",
    "#Load and Clean data for merging\n",
    "cleaned_dataset1 = complete_set1(file_path)\n",
    "cleaned_dataset1['county'] = cleaned_dataset1.apply(lambda row: get_county(row['latitude'], row['longitude']), axis=1)\n",
    "cleaned_dataset1 = county_filter(cleaned_dataset1, county_names)\n",
    "\n",
    "cleaned_dataset1.to_csv('cleaned_dataset1.csv', index=False)\n",
    "print(cleaned_dataset1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading of Second Data Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_set2(susbset_list):\n",
    "    full_data = pd.DataFrame()\n",
    "    for idx, file_name in enumerate(susbset_list):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_name)\n",
    "        if idx == 0:\n",
    "            full_data = df\n",
    "        else:\n",
    "            full_data = pd.concat([full_data, df], ignore_index=True)\n",
    "\n",
    "    # Clean the DataFrame\n",
    "    full_data = full_data.dropna(subset=['Address', 'Bedrooms'])\n",
    "\n",
    "    #Get coordinates (Longitude and Latitude)\n",
    "    full_data['latitude'], full_data['longitude'] = zip(*full_data.apply(lambda row: get_coordinates(row['Address'], row['City'], row['Zip'], row['State']), axis=1))\n",
    "    full_data['latitude'] = full_data['latitude'].round(2)\n",
    "    full_data['longitude'] = full_data['longitude'].round(2)\n",
    "\n",
    "    #Get counties\n",
    "    full_data['county'] = full_data.apply(lambda row: get_county(row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "    return full_data\n",
    "\n",
    "def clean_set2(full_data, county_names, aggregation_functions):\n",
    "    filtered_data = county_filter(full_data,county_names)\n",
    "    cleaned_dataset2 = data_aggregation(full_data,aggregation_functions)\n",
    "\n",
    "    return cleaned_dataset2\n",
    "\n",
    "\n",
    "\n",
    "# List of subsets to join for dataSet\n",
    "subset_list = ['train.csv', 'test.csv']\n",
    "\n",
    "# Define County codes wanted\n",
    "county_names = {\n",
    "    '037': 'Los Angeles',\n",
    "    '073': 'San Diego',\n",
    "    '059': 'Orange',\n",
    "    '065': 'Riverside',\n",
    "    '071': 'San Bernardino'\n",
    "}\n",
    "\n",
    "# Agregate data for merging\n",
    "# Define aggregation dictionary for desired columns\n",
    "aggregation_functions = {\n",
    "    'Sold Price': 'mean',  # Average price of homes - Use mean to get average of values in same area\n",
    "    'Year built': 'mean',\n",
    "    'Bedrooms': 'sum',  # Total number of bedrooms - Use sum to add values in same area\n",
    "    'Bathrooms': 'sum',\n",
    "    'Full bathrooms': 'sum',\n",
    "    'Total interior livable area': 'mean', #Average house area\n",
    "    'Elementary School Distance': 'mean', # Average distance to schools\n",
    "    'Middle School Distance': 'mean', # Average distance to schools\n",
    "    'High School Distance': 'mean', # Average distance to schools\n",
    "    'Last Sold Price': 'mean'  #Average house price\n",
    "}\n",
    "\n",
    "# Get complete set 2 (join subsets, find coordinates and counties)\n",
    "full_data = complete_set2(subset_list)\n",
    "\n",
    "# Clean data\n",
    "cleaned_dataset2 = clean_set2(full_data, county_names)\n",
    "\n",
    "#Save csv of completed processed data set to current directory\n",
    "cleaned_dataset2.to_csv('cleaned_dataset2.csv', index=False)\n",
    "print(cleaned_dataset2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
